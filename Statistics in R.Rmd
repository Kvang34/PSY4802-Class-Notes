---
title: "statistics in R"
author: "Amanda Mae Woodward"
date: "2024-11-05"
output: html_document
---

# Learning Outcomes:
By the end of today's class, students should be able to: 
- obtain descriptive statistics in R
- conduct common parametric analyses in R
- conduct common nonparametric analyses in R

**Disclaimer:** Covering every type of analysis in R could be an entire course by itself. Today, we'll cover **some** analyes you can do. If there are additional analyses you'd like to cover, please let me know and I'm happy to upload supplemental code or cover it in a later class (there is flexibility in the last couple of weeks!). 

Additionally, we will **not** cover interpretations in depth in this class. The goal is to teach you how to use R to run the tests, and adding interpretations for each test could make this into several semester long courses. However, if you have questions about how to interpret statistics, please let me know and I can adjust our course material. I am happy to talk about interpretations in office hours, or you will learn about them in your statistics courses.

We'll simulate data to use throughout today's class:

To do this, we'll use a couple of functions we've used before: `set.seed()`, `rep()`, and `sample()`

**Question**: What research question do we want to focus on? 

Research questions: 
1) What factors are related to the number of penguins born in a year?
```{r}
# number of observational variables(n) : 1000

# set.seed(n) ensure that all of our number are at the same location
set.seed(137)
# rnorm(1) 

# create a data set
penguinsBorn <- sample(14400:(14400*10), size = 1000, replace = TRUE)

# Indicator variable for year
year <- as.factor(1025:2024)
# this tells us that we have collected the data from 1025 to 2024

# Creating the variables
temperature<- sample(-40:50, 1000, replace=TRUE)#quantitative

fishAvailable<- sample(50000:1000000, 1000, replace=TRUE)#quantitative

polarbearPresence<- sample(c("yes", "no"), 1000, replace=TRUE, prob = c(.65,.35))#qualitative (yes or no); want a prob of 65 be true and 34 be false

emotionsOfPenguins<- sample(c("dangerously depressed", "very sad", "moderately sad", "neutral", "moderately happy", "very happy", "dangerously happy"), 1000, replace=TRUE)#ordinal

region<- sample(c("Southern Africa", "Oceania", "Antarctica", "South America"), 1000, replace= TRUE) #qualitative

ratioOfPenguins<- rnorm(1000, mean=.5, sd= .1) #quantitative (0=all male; 1= all female)
```

```{r}
# Create a data frame
penguinsData <- cbind.data.frame(year, penguinsBorn, emotionsOfPenguins, fishAvailable, polarbearPresence, ratioOfPenguins, region, temperature)

```

### Learning Outcome 1: Obtaining descriptive statistics in R
We've gone through some of these already, but I want to make sure we're on the same page. For descriptive statistics, we'll mostly focus on the measures of central tendency and measures of variability. 

#### Central Tendency

##### Mean 
```{r}
# mean(dataName$columnName)
mean(penguinsData$fishAvailable)

```
##### Median
```{r}
# median(dataName$columnName)
median(penguinsData$fishAvailable)

```
##### Mode
```{r}
# mode(dataName$columnName)
mode(penguinsData$fishAvailable)

# install the modeest library
library(modeest)
mfv(penguinsData$fishAvailable)
```

```{r}
# using pipes
# you can summary by region as well using pipe by using 'group_by' function
library(tidyverse)
penguinsData %>%
  group_by(as.factor(region)) %>%
  summarize(meanFish = mean(fishAvailable), mediumFish = median(fishAvailable))

```

#### Variability

##### Range
```{r}
# range(dataName$columnName)
range(penguinsData$fishAvailable)

# You can calculate the range by doing this 
range(penguinsData$fishAvailable)[2] - range(penguinsData$fishAvailable)[1]

# or 

999762 - 50533 

```

##### Interquartile Range
```{r}
# IQR(dataName$columnName)
IQR(penguinsData$fishAvailable)

```

##### Standard Deviation
```{r}
# sd(dataName$columnName)
sd(penguinsData$fishAvailable)
```

##### Variance
```{r}
# var(dataName$columnName) or sd(dataName$columnName)^2

var(penguinsData$fishAvailable)

# or 

sd(penguinsData$fishAvailable)^2
```

##### Summary
```{r}
# summary(dataName$columnName)
summary(penguinsData$fishAvailable)
```

#### Z Score

The other thing that we'll put in this section is how to create a z score in your data. This allows us to view one score relative to others, even if they are collected from different distributions

```{r}
# scale(dataName$columnName) function 

scale(penguinsData$fishAvailable)
```

##### Learning Outcome 1 Practice
1) calculate the mean, median, and mode for any data in the our Dataset
```{r}
# mean
mean(penguinsData$penguinsBorn)

# median 
median(penguinsData$penguinsBorn)

# mode
mfv(penguinsData$penguinsBorn)


```

2) what do you notice about these scores? (are they the same? different?) *They are different*

3) create z scores for any data in our Dataset. Interpret what participant 3's z score means. 
```{r}
# zscore for temperature 
scale(penguinsData$penguinsBorn)
```

```{r}
# The zscore for participant 3 is 
scale(penguinsData$penguinsBorn)[3]

# This tells us that the zscore for participant 3 is -0.06. This means that participant 3 was pretty close to the mean, about -.06 below the mean. 
```

 *Challenge* Graph your data and include the mean median and mode on the graph 
```{r}
ggplot(penguinsData, aes(penguinsBorn))+ 
  geom_histogram(fill= "grey", bins=100)+
  geom_freqpoly(bins= 100)
```

```{r}
ggplot(penguinsData, aes(penguinsBorn))+ 
  geom_boxplot()

```

### Learning Outcome 2: Conduct common parametric analyses in R

Now that we have covered some descriptive statistics, we'll talk about parametric ones. Parametric statistics are those that rely on assumptions to make inferences from the sample to the population. We'll go through correlations, t-tests, regression, and ANOVA. We'll go through nonparametric tests, or those that rely on less assumptions, in the next section. 

#### Pearson correlation
We'll practice running correlations using the dataset above. To do this, we'll look at the correlation between fishavaible and temperature 
`cor(x, y)`
```{r}
cor(penguinsData$fishAvailable, penguinsData$temperature)

# The correlation is -0.03. We do not have a linear relationship because it is negative.
```
**Note:** It's great that we can see the correlation between these two measures, but we don't have any additional information, ie information related to significance.We can use another function, `cor.test()`, to get information about significance.
`cor.test(x,y)`
```{r}
cor.test(penguinsData$fishAvailable, penguinsData$temperature)

# This give that t= -1.05, df=998, p-value= 0.29, cor= -0.03
```
graph
```{r}
ggplot(penguinsData, aes(fishAvailable, temperature))+ 
  geom_point()+
  theme_classic()
```

We can change whether we our conducting a one tailed or a two tailed test by including an additional argument "alternative." It defaults to a two tailed test, but we can specify a one tailed test in either direction ("greater" or "less") 
```{r}
# directional hypothesis 

cor.test(penguinsData$fishAvailable, penguinsData$temperature, alternative = "greater")

#  it states that the correlation is 0 - "true correlation is greater than 0"
```

### Extra Code about Correlation Tables 
`cor()` can also be used to create correlation matrices, but need to create a dataframe that is just the variables you'd like to use. 
`cor(data)`
```{r}
penguinsQuant <- penguinsData %>% 
  select(fishAvailable, temperature, penguinsBorn)
cor(penguinsQuant)

# When we run the code, it shows that there's no correlation between our variables 
```

#### t-tests
We can run a variety of t-tests using the same function `t.test()`. 

##### one sample t-test
A one sample t test can be computed by specifying mu in the arguments. 
`t.test(variable, mu)`
```{r}
# t-test compare the average difference 

# mu is the population mean

t.test(penguinsData$temperature, mu= 0)

# When we run the code, it tells us that we did a one sample t-test. It give us the t-value, df, and p-values. The mean of the sample is 4.4

# Alternative hypothesis 
t.test(penguinsData$temperature, mu= 0, alternative = "less")
# p-value of 1 tells us the possibility of seeing our data if it is true. It tells us that we will possibly see our mean if our hypothesis is true. 

```

##### two samples t-test
There are two ways we can use this function when we have two variables (independent or paired). The first is to type our x and y variables in as we did in the correlation function above. 
```{r}
# The two ways to do it:
# t.test(DV ~ IV, data = dataName)
# t.test(variableName1, variableName2)

t.test(temperature ~ polarbearPresence, data= penguinsData)
# Not statistically significant because p-value = 0.99

```
You'll notice that the top of the t-test output says "Welch's Two sample t-test." This R function automatically assumes that the variances of each group are unequal. If we wanted to run a traditional paired-samples t-test, we need to include another argument. 

OR
we can type them in as a formula in R. Formulas typically take the form y ~ x. To show you this example, I need to reformat our wide data to long data (using what we did earlier in class!)
`t.test(dependent variable ~ indepedent variable, data= dataframe)`

```{r}
t.test(temperature ~ polarbearPresence, data= penguinsData, var.equal= TRUE)

```
If our data were dependent between observations, we'll run a *paired samples t test*. The code looks pretty similar to above, but we'll use an additional argument. 

let's use the sleep datset as an example
```{r}
summary(as.factor(penguinsData$polarbearPresence))

# If they are not the same number, we can't do a peair t-test

data(sleep)
t.test(extra ~ group, data=sleep, pair= TRUE)
```

Finally, we some times run one tailed vs two tailed tests, just like we did with the correlations. 
```{r}

```

##### Correlation and T-test practice
1. Open the mtcars dataset. Find the correlation between mpg and hp
```{r}
data(mtcars)
cor(mtcars$mpg, mtcars$hp)
```

2. Conduct a significance test to determine if displacement and miles per gallon significantly correlated. 
```{r}
cor.test(mtcars$disp, mtcars$mpg)
# We have a significant correlation because it say there is a true correlation
```

3. Conduct a two-tailed t-test examining whether the average mpg differs by transmission (am). 
```{r}
t.test(mpg ~ am, data= mtcars)
```

4. Conduct a one-tailed t-test examining whether the average displacement(disp) differs engine shape (vs). Specifically, test whether straight engines result in higher displacements.
```{r}
t.test(disp ~ vs, data= mtcars, alterative = "less")
# We do less than because straight engine is 1 and tells us that straight engine have 132
```


#### regression
Back to the simulated Dataset we made. The code for a linear regression is really similar (ie identical)  to what we used for t-tests.
`lm(DV ~ IV, data)`
```{r}
lm(penguinsBorn ~ fishAvailable, data= penguinsData)
```
I tend to save my linear models because it allows me to do a few useful things:
Just like we used   `summary()` to get a summary of our data, we can use the same function to learn more about our models
```{r}
penguinBornRegression<- lm(penguinsBorn~ fishAvailable, data= penguinsData)

summary(penguinBornRegression)
```
`str()` is a function that allows us to learn about the structure of our model. We can use this to get specific pieces of information, or additional information that "underlies" our model (eg residuals and fitted values)
```{r}
str(penguinBornRegression)

penguinBornRegression$coefficients

penguinBornRegression$model$penguinsBorn #can use more than one dollar sign 
```

**Multiple Regression**
We can include additional factors and interaction terms to our models: 

```{r}
penguinsBornMult<- lm(penguinsBorn ~ fishAvailable + temperature, data= penguinsData)

summary(penguinsBornMult)
```
Using * instead of + will include both the individual predictors AND their interactions
```{r}
penguinsBornMultInteraction<- lm(penguinsBorn ~ fishAvailable * temperature, data= penguinsData)

summary(penguinsBornMultInteraction)
```
 
The : can be used instead of + to include an interaction in your model
```{r}
lm(penguinsBorn~ fishAvailable + polarbearPresence+temperature + fishAvailable:temperature, data=penguinsData)

lm(penguinsBorn~ fishAvailable*temperature*polarbearPresence, data= penguinsData)
```

The class of our data and the way data are entered matter for regression models. 
let's consider condition:

Data don't really look continuous. We can change age to a factor. This will influence our output.  

We may also need to change the reference level for factors.
`relevel(dat$age, ref="x")`
```{r}
polarBearReg<- lm(penguinsBorn~ polarbearPresence, data=penguinsData)
summary(polarBearReg)

```

**Anova**
There are several ways you can get Anova results in R. There are differences in the ways that they handle interactions, but they are used in the same way. We can use `aov()`, `anova()`, and `Anova()`

```{r}
"ANOVA test"
# install.packages("car")
library(car)
Anova(lm(penguinsBorn~ polarbearPresence, data=penguinsData))

polarBearANOVA<-aov(penguinsBorn~ polarbearPresence, data=penguinsData)
summary(polarBearANOVA)
```

```{r}
"post hoc test"
TukeyHSD(polarBearANOVA) # don't really need a posthoc test here

EmotionANOVA<- aov(penguinsBorn~ emotionsOfPenguins, data=penguinsData)
summary(EmotionANOVA)
TukeyHSD(EmotionANOVA)
```

```{r}
"effect size"
# install.packages("lsr")
library(lsr)
etaSquared(EmotionANOVA)
```

 more predictors!
```{r}
EmotBearANOVA<-aov(penguinsBorn~ emotionsOfPenguins* polarbearPresence)
summary(EmotBearANOVA)
```

```{r}
"multiple comparisons"
multcompar<-TukeyHSD(EmotBearANOVA)
multcompar$`emotionsOfPenguins:polarbearPresence`#allows you to just look at one set of multiple comparisons

TukeyHSD(EmotBearANOVA, "polarbearPresence")
```

```{r}
"effectsize"

```
 
#### Regression and ANOVA practice
1. Use the mtcars dataset and create a linear model that predicts mpg from cylinder (cyl) and displacement. Print the results
```{r}

summary(lm(mpg~ cyl+ disp, data=mtcars))

# The class for cyl is not a factor, so we use as.factor() function because the output is different 

summary(lm(mpg~ as.factor(cyl)+disp, data=mtcars)
        
```

2. Create the same model, but include an interaction term. 
```{r}
summary(lm(mpg~ as.factor(cyl)*disp, data=mtcars))

```

3. Run an ANOVA predicting hp from the transmission variable. 
```{r}
summary(aov(hp~am, data=mtcars))
```

###Learning Outcome 3: Nonparametric analyses in R
Nonparametric analyses are run similarly to their parametric versions. In the interest of time, we'll talk about biserial correlations, spearman correlations, Wilcoxon sign rank tests, and binomial tests. 

**biserial correlations**
Biserial correlations involve a binary outcome and a continuous variable. To run one in R, we need the ltm package. 
```{r}
# install.packages("ltm")
library(ltm)
```
the function is `biserial.cor(continuous, binary)`
```{r}
biserial.cor(penguinsData$penguinsBorn, penguinsData$polarbearPresence, level=2)

library(ggplot2)
ggplot(penguinsData, aes(penguinsBorn, polarbearPresence))+ geom_point()+geom_smooth(method="lm")
```
Mathematically, this is the same as the pearson's version. 
```{r}
cor(penguinsData$penguinsBorn, penguinsData$polarbearPresence)
```

**spearman's rho**
We can calculate spearman's rho and kendall's tau the same way. We just need to use the "method" argument for `cor.test()`
```{r}
levels(as.factor(penguinsData$emotionsOfPenguins))
penguinsData$emotionsQuant<- car::recode(penguinsData$emotionsOfPenguins, recodes = "'dangerously depressed'= 1; 'dangerously happy'= 7; 'moderately happy' = 5; 'moderately sad'=3;'neutral'=4; 'very happy'= 6; 'very sad'= 2")
cor.test(penguinsData$penguinsBorn, penguinsData$emotionsQuant,method="spearman")

# "Cannot compute exact p-value with ties" this will appear when you run the code because it is unable to calculate all the p-value and would rank the data in order

```

```{r}
# Another way is using the kendall, work for ordinal data 

cor.test(penguinsData$penguinsBorn, penguinsData$emotionsQuant, method="kendall")

```

**Wilcoxon sign rank test**
This is the nonparametric version of the t-test. It has the same arguments. We'll do one as an example. 
`wilcox.test()`
```{r}
# wilcox.test(DV ~ IV)

wilcox.test(penguinsData$penguinsBorn~ penguinsData$polarbearPresence)
# "true location shift is not equal to 0" tells us that it is a two-tail t-test, it provide the w-value, p-values 
# We would use this test when our population is normally distributed, equal variance, and the sample are randomly selected 
# We can use this if a t-test does not work 

```
**chi squared test** 
compare the freq of nominal data. You can ask questions like: does the freq of polarbear presense differ from expected? 

chiquare goodness of fit test allows us to answer this question. to do this, we're going to use the function `chisq.test()`
```{r}
# we started with ...
# chisq.test(penguinsData$polarbearPrense, p= c(.5,.5)), but the variable was not a factor, so we have to recode 

penguinsData$polarbearPresenceQuant <- recode(penguinsData$polarbearPresence, '"yes"=1;"no"=0')
summary(as.factor(penguinsData$polarbearPresenceQuant))
chisq.test(c(.344, .656), p= c(.5,.5))

```
# with a probability of 50-50 of how many polarbear live in N and S
# we got the number from the summary() and divide it by 1000
# however, if we expect the polar bear to live in only the south pole, then the probability would be like (0,100)

**binomial tests**
We use binomial tests to determine if something happens more often than chance. The function is binom.test and it has the following arguments: 
`binom.test(successes, totalScores, probability)`
```{r}

# if we have a true and false test and got 12/20 questions right. We have a probability of 50 because we have two possible answers (T/F)
binom.test(12, 20, .5)

# When we run the code, it tells us that you will get 60% on the test by chance 

```

for instance, if we have 10 true/false statements, and get 6 right. Does this differ from chance? 
```{r}
binom.test(6, 10,.5)
```
This is a two-tailed test, but can also do one tailed by specifying the alternative. 

20 questions, 5 choices, and want to know probability of getting 14 correct
```{r}
binom.test(14, 20, .2)
```


#### Learning Outcome 3 practice: 
1. using the mtcars dataset, run a correlation to determine the relationship between engine shape (vs) and hp. What test did you run and why? 
```{r}
data(mtcars)
class(mtcars$vs)
class(mtcars$hp)

# use biserial test because one of the variable is continious 
biserial.cor(mtcars$hp ~ mtcars$vs, level=2)
```

2. Run a wilcoxon sign rank test to determine if cylinder and gears have different means. 
```{r}

#wilcox.test(cyl~gear, data= mtcars)

wilcox.test(mtcars$cyl, mtcars$gear)


# nonparametric version of an ANOVA
kruskal.test(cyl~gear, data=mtcars)

```

3. Run a binomial test to determine if the number of cars with manual transmission differs from chance. (hint: use the ? feature to learn more about the dataset.)

```{r}
# chance is by 5 
# total obs is 32

summary(as.factor(mtcars$am))
table(mtcars$am)
binom.test(12,32,.5)

```
